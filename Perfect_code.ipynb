{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Dowloading Pretrained GPT-2 Small Model and Tokenizer from HuggingFace Transformers Library  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2',use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer encodes the text into Ids that we need to send to the model\n",
    "ids = tokenizer.encode(\"who is a member of the National Security Council,and is a member of\",return_tensors = 'tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.As we want to generate Three sentences we have to pass three sentences same time so helps in Parallel Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list()\n",
    "for i in [\"She was really good girl\",\"She was really good girl\",\"She was really good girl\"]:\n",
    "    ids = tokenizer.encode(i,return_tensors = 'tf')\n",
    "    x.append(ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Sentences after Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3347,  373, 1107,  922, 2576],\n",
       "       [3347,  373, 1107,  922, 2576],\n",
       "       [3347,  373, 1107,  922, 2576]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Constructing a TensorFlow Graph from the custom Tensorflow Model Code so we can use it with TF.Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callable = tf.function(model.call)\n",
    "concrete_function = callable.get_concrete_function(tf.TensorSpec([None,None], tf.int32, name=\"input_ids\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only interested in the last word predicted \n",
    "logits = concrete_function(np.array(x))[0][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are decoding the sample prediction sentence.\n",
    "tokenizer.decode([np.argmax(concrete_function(np.array(x))[0][0][-1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now problem with above saved model is it generates an array of length 50,527 words per every request those are logits but we need only 1 best word and other disadvantage is payload as number of words increases payload also increases so to avoid that we need to select best word in the server itself and send the top sentences on every request to do that we need to do post processsing in the graph itself after prediciton below code does it for us.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Function Performs Top_k sampling </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def Top_k(logits,temperature=0.9,k=50):\n",
    "    'Removes all the least considered words and keeps only the top k words'\n",
    "    logits = tf.divide(logits,temperature)\n",
    "#    Take top 50 and then divide the logits by its temperature so we can make low probalabe values even more probabale and less probable values even less probable\n",
    "    values,_=tf.math.top_k(logits, k=50, sorted=True,name = \"top50\") #selecting Top 50 logits\n",
    "#     select the minimum value from each axis and divide with temperature\n",
    "    minimum_values = values[:,-1][:,tf.newaxis]\n",
    "#     Now make all the values below this -1e*10 in the logits this happens by broadcasting \n",
    "    return tf.where(logits<minimum_values,tf.ones_like(logits, dtype=logits.dtype) * -1e10,logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Function to perform Top_p or Neucleus Sampling After Top_k sampling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def Top_p(logits, p=0.95):\n",
    "    \"\"\"Nucleus sampling This will only consider the words utill the specified probabilities\"\"\"\n",
    "    batch = 3\n",
    "    sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)\n",
    "    cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "    indices = tf.stack([\n",
    "        tf.range(0, batch),\n",
    "        # number of indices to include\n",
    "        tf.maximum(tf.reduce_sum(tf.cast(cumulative_probs <= p, tf.int32), axis=-1) - 1, 0),\n",
    "    ], axis=-1)\n",
    "    min_values = tf.gather_nd(sorted_logits, indices)[:,tf.newaxis]\n",
    "    return tf.where(\n",
    "        logits < min_values,\n",
    "        tf.ones_like(logits) * -1e10,\n",
    "        logits,\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=int64, numpy=\n",
       "array([[2474],\n",
       "       [ 553],\n",
       "       [ 553]], dtype=int64)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.random.categorical(Top_p(Top_k(concrete_function(np.array(x))[0][:,-1],.9,50)),num_samples = 1)[:,-1,tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Function Generates New Sentences and based on Given Input ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_output(input_ids,top_k = 45,len = 7,top_p = .9,temp = .9):\n",
    "    tokens = input_ids\n",
    "    outputs = tf.random.categorical(Top_p(Top_k(concrete_function(tokens)[0][:,-1],temp,top_k)),num_samples = 1)\n",
    "    tokens = tf.concat([tokens,tf.cast(outputs[:,-1,tf.newaxis],dtype = tf.int32)],1)\n",
    "    for i in tf.range(len-1):\n",
    "        tf.autograph.experimental.set_loop_options(\n",
    "        shape_invariants=[(outputs, tf.TensorShape([None,None])),((tokens, tf.TensorShape([None,None])))])\n",
    "        x = tf.random.categorical(Top_p(Top_k(concrete_function(tokens)[0][:,-1],temp,top_k),top_p),num_samples = 1)\n",
    "        outputs = tf.concat([outputs,x],1)\n",
    "        tokens = tf.concat([tokens,tf.cast(outputs[:,-1,tf.newaxis],dtype = tf.int32)],1)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', and the young girl, she'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(get_output(np.array(x))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generates Graph from above Get Output Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = get_output.get_concrete_function(tf.TensorSpec([None,None], tf.int32, name=\"input_ids\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 7), dtype=int64, numpy=\n",
       "array([[   11,   257, 16365,   351,   257, 16365,   338],\n",
       "       [  290,   257,  2933,    11,   475,   673,  1422],\n",
       "       [   11,   508,    11,  1864,   284,   607,    11]], dtype=int64)>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample output\n",
    "function(np.array(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model into tensorflow SavedModel Format so It can be used In TensorFlow serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Top_sentences_best/001\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model,'Top_sentences_best/001',signatures = function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = tf.saved_model.load('Top_sentences_best/001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_0': <tf.Tensor: shape=(3, 7), dtype=int64, numpy=\n",
       " array([[ 508,  373,  531,  284,  423,  587, 2407],\n",
       "        [  11,  508,  550,  587,  612,  329,  617],\n",
       "        [ 290,  257, 4957,  508,  547, 3595,   13]], dtype=int64)>}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_.signatures['serving_default'](input_ids = tf.constant(np.array(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list()\n",
    "for i in [\"There lived a old witch\",\"There lived a old witch\",\"There lived a old witch\"]:\n",
    "    ids = tokenizer.encode(i,return_tensors = 'tf')\n",
    "    x.append(ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tf.constant(np.array(x),dtype = tf.int32)\n",
    "for i in range(10):\n",
    "    outputs = tf.random.categorical(Top_p(Top_k(concrete_function(tokens)[0][:,-1]),0.95),num_samples = 1)\n",
    "    tokens = tf.concat([tokens,tf.cast(outputs,dtype = tf.int32)],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There lived a old witch, but it was no longer able to live.\n",
      "There lived a old witch named P. A. Taney who took over\n",
      "There lived a old witch who lived with the white man in the castle.\n"
     ]
    }
   ],
   "source": [
    "for i in tokens:    \n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "callable = tf.function(model.call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_function = callable.get_concrete_function(tf.TensorSpec([None,None], tf.int32, name=\"input_ids\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_output(np.array(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = get_output.get_concrete_function(tf.TensorSpec([None], tf.int32, name=\"input_ids\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = function(np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There lived a old witch who lived in a castle, and a man of\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There lived a old witch, who died of her illness. And what the'"
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Top_sentences_1\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model,'Top_sentences_1',signatures = function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = tf.saved_model.load('Top_sentences_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=model_.signatures['serving_default'](input_ids=tf.constant(np.array(x)))['output_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There lived a old witch with her husband who used to tell her about her\n",
      "There lived a old witch that lived in the house of the witches and the\n",
      "There lived a old witch and her husband who would be executed for witchcraft.\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "server_url = \"http://localhost:8501/v1/models/TypeAhead:predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_json = json.dumps({\"instances\":np.array(x).tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"instances\": [[1858, 5615, 257, 1468, 16365], [1858, 5615, 257, 1468, 16365], [1858, 5615, 257, 1468, 16365]]}'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(server_url,data = input_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"predictions\": [[508, 550, 1716, 257, 16365, 13, 1375], [508, 5615, 287, 262, 17012, 286, 465], [3706, 9074, 13, 376, 7737, 508, 550]\\n    ]\\n}'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.constant(5).as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " who had become a witch. She\n",
      " who lived in the basement of his\n",
      " named Mrs. Fanny who had\n"
     ]
    }
   ],
   "source": [
    "for i in np.array(response.json()['predictions']):\n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list()\n",
    "for i in [\"Once upon a time there lived a king without fear whatsoever\",\"Once upon a time there lived a king without fear whatsoever\",\"Once upon a time there lived a king without fear whatsoever\"]:\n",
    "    ids = tokenizer.encode(i,return_tensors = 'tf')\n",
    "    x.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 7454,  2402,   257,   640,   612,  5615,   257,  5822,  1231,\n",
       "          3252, 16014]],\n",
       "\n",
       "       [[ 7454,  2402,   257,   640,   612,  5615,   257,  5822,  1231,\n",
       "          3252, 16014]],\n",
       "\n",
       "       [[ 7454,  2402,   257,   640,   612,  5615,   257,  5822,  1231,\n",
       "          3252, 16014]]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50257,), dtype=float32, numpy=\n",
       "array([-74.626976, -75.38203 , -82.51564 , ..., -87.272064, -83.40596 ,\n",
       "       -77.62991 ], dtype=float32)>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function(np.array(x))[0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5, 50257), dtype=float32, numpy=\n",
       " array([[ -37.276787,  -36.41671 ,  -41.166496, ...,  -44.37204 ,\n",
       "          -43.39425 ,  -37.471313],\n",
       "        [-113.54544 , -113.27844 , -122.17086 , ..., -118.59855 ,\n",
       "         -120.95412 , -117.01207 ],\n",
       "        [-106.12589 , -104.31913 , -108.76358 , ..., -110.14669 ,\n",
       "         -107.425575, -105.50971 ],\n",
       "        [ -84.54696 ,  -82.34526 ,  -88.21153 , ...,  -88.45912 ,\n",
       "          -89.08511 ,  -85.38495 ],\n",
       "        [ -87.48384 ,  -89.14494 ,  -94.514565, ...,  -98.71225 ,\n",
       "          -96.36986 ,  -91.682816]], dtype=float32)>,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(np.array(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([np.argmax(model_.signatures['serving_default'](ids[0])['output_0'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',. of; and for in:.\" that to who\\n but—,\"... as the about at ; from or (!… on [ — which-- against.... among except. because upon - a before with\" -- – when over? save'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(model_.signatures['serving_default'](ids[0])['output_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8558431>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(tf.math.top_k(model(ids)[0][0][-1], k=50, sorted=True,name = \"top50\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
